{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocesamiento de imágenes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbj4I-J6gsUc"
      },
      "source": [
        "Para poder hacer el proceso de descarga hay que poner el entorno de ejecución al de TPUs, que nos dan casi 80GBs de memoria. Para ello seguimos los siguientes pasos:\n",
        "Entorno de ejecución > Cambiar tipo de entorno de ejecución > Aceleración por hardware a TPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvBoIxLgqAIB"
      },
      "source": [
        "from google.colab import drive  # Se conecta a drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYzcGEA3qZH5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylaBXDU4qdQq"
      },
      "source": [
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from skimage.io import imread\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSxHzutNqu4q"
      },
      "source": [
        "# Carga de datos (dataset AVA)\n",
        "\n",
        "Primero se descargan las 255.000 imágenes del dataset AVA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTdvM6Bwq87L"
      },
      "source": [
        "# Descarga de Imagenes\n",
        "!wget 'https://pruebasaluuclm-my.sharepoint.com/:u:/g/personal/fernando_rubio_uclm_es/ERGaDwGaYsVLg-5qpH4Sw_gBjet7G8WQAV2ijrMWu5bl9w?download=1'\n",
        "!wget 'https://pruebasaluuclm-my.sharepoint.com/:u:/g/personal/fernando_rubio_uclm_es/EaNgOn3PuNdGlMx_qP6eshYBGYkcLuZtlrhphrdQcY_Ezg?download=1'\n",
        "!wget 'https://pruebasaluuclm-my.sharepoint.com/:u:/g/personal/fernando_rubio_uclm_es/EdRxjrziO81HiCD9271rg9MBB4erjdKj3DFNvKUxcSYfbw?download=1'\n",
        "!wget 'https://pruebasaluuclm-my.sharepoint.com/:u:/g/personal/fernando_rubio_uclm_es/EZntIFgpkyhFhfBGG5Qfz0oBvwbTMJZZEshbaVPoRtqKwg?download=1'\n",
        "\n",
        "# Descarga de Ficheros\n",
        "!wget 'https://pruebasaluuclm-my.sharepoint.com/:u:/g/personal/fernando_rubio_uclm_es/ESuuKua4j2hLvkFMxLNw84YBtTFAWu8uLxmXB--Pf-VwVQ?download=1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_y3tLK6rBAA"
      },
      "source": [
        "Se extraen las imágenes y se guardan en la carpeta AVADataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0exeEkFq_Fs"
      },
      "source": [
        "# Se extraen los archivos\n",
        "# Imagenes\n",
        "!mv 'ERGaDwGaYsVLg-5qpH4Sw_gBjet7G8WQAV2ijrMWu5bl9w?download=1' AVADataset.partaa\n",
        "!mv 'EaNgOn3PuNdGlMx_qP6eshYBGYkcLuZtlrhphrdQcY_Ezg?download=1' AVADataset.partab\n",
        "!mv 'EdRxjrziO81HiCD9271rg9MBB4erjdKj3DFNvKUxcSYfbw?download=1' AVADataset.partac\n",
        "!mv 'EZntIFgpkyhFhfBGG5Qfz0oBvwbTMJZZEshbaVPoRtqKwg?download=1' AVADataset.partad\n",
        "!cat AVADataset.part* > AVADataset.tar.gz\n",
        "!rm -r AVADataset.part*\n",
        "!tar -xzf AVADataset.tar.gz\n",
        "!rm -r AVADataset.tar.gz\n",
        "\n",
        "# Ficheros\n",
        "!mv 'ESuuKua4j2hLvkFMxLNw84YBtTFAWu8uLxmXB--Pf-VwVQ?download=1' AVA_Dataset.zip\n",
        "!unzip -q AVA_Dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3a6HYsnrL8Y"
      },
      "source": [
        "Se comprueba la cantidad de imágenes que hay en el directorio `AVADataset/`. En total debería de haber 255.353 (si se está trabajando con el dataset AVA completo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh08FzTlriaT"
      },
      "source": [
        "len(os.listdir(\"AVADataset/\"))  # Se comprueba que se han descargado todas las imagenes: 255.353"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kDqKOAkrl0R"
      },
      "source": [
        "Se descarga un archivo pandas con la información de las imágenes del dataset AVA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esDFwJwcrkkU"
      },
      "source": [
        "# Descarga del archivo con informacion de la bbdd\n",
        "!wget 'https://pruebasaluuclm-my.sharepoint.com/:u:/g/personal/fernando_rubio_uclm_es/EZ8t-mcdR9tEjxPfzgmjbeIBDbs6PDnYbGsN2MOySInppA?download=1'\n",
        "!mv 'EZ8t-mcdR9tEjxPfzgmjbeIBDbs6PDnYbGsN2MOySInppA?download=1' AVA_subset.zip\n",
        "!unzip -q AVA_subset.zip\n",
        "!wget 'https://pruebasaluuclm-my.sharepoint.com/:u:/g/personal/fernando_rubio_uclm_es/ESUYgue7Bv5DshQbI_QGYUoBwycoBSrw-0NkXzK-Qu71mA?download=1'\n",
        "!mv 'ESUYgue7Bv5DshQbI_QGYUoBwycoBSrw-0NkXzK-Qu71mA?download=1' AVA_info.pklz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3LBZJ79rrOv"
      },
      "source": [
        "Se carga en un archivo pandas la bbdd. El `id` se corresponde con el nombre de las imágenes, y `Class` con el valor de la variable clase (0 = mala calidad, 1 = buena calidad)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czR06o1xrqz3"
      },
      "source": [
        "import gzip\n",
        "import pickle\n",
        "file_pickle = gzip.open('AVA_info.pklz','rb',2)\n",
        "data = pd.read_pickle(file_pickle, compression=None)  # Se carga el dataframe\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ijsj0RWrvnF"
      },
      "source": [
        "Se construye una bbdd solo con la columna id (contiene la direccion de los archivos) y la clase (0 o 1). Son las únicas columnas que se van a utilizar en este caso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD-j_c4YrvI0"
      },
      "source": [
        "data = data[['id', 'Class']]\n",
        "data['id'] = data['id'].apply(lambda x: 'AVADataset/' + str(x) + '.jpg')\n",
        "data['Class'] = data['Class'].apply(lambda x: str(x))\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PSl_mHir0In"
      },
      "source": [
        "Se carga la X y la y a partir de la bbdd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIpdEZGgr0nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c782a3-cb58-4341-9c33-46102b40efea"
      },
      "source": [
        "# Se define la X (direccion de las imagenes)\n",
        "X = data['id'].to_numpy()\n",
        "# Se define y (0 1)\n",
        "y = data['Class'].to_numpy()\n",
        "y = y.astype(np.float)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=2)\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((255353,), (255353, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzFr7mmHqlWG"
      },
      "source": [
        "# Preprocesamiento\n",
        "\n",
        "Hay que preprocesar las imágenes. Para ello se extraen 9 parches y cada uno de estos parches se pasa por la red MobileNet, obteniendo 9 vectores de características de tamaño 1024.\n",
        "\n",
        "Estos 9 vectores de características serán la entrada de los 3 modelos basados en mecanismos de atención y redes LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vY4xb26sAN-"
      },
      "source": [
        "Se define el generador de parches. Recibe la dirección de la imagen a tratar junto con su variable clase y devuelve los 9 parches y la variable clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY3nzokNsCk5"
      },
      "source": [
        "class PatchGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, files, labels, target_size = (224,224), \n",
        "                 batch_size = 32, nb_crops=3):\n",
        "        self.files = files              # Direccion de las imagenes\n",
        "        self.n_files = len(self.files)  # Cantidad de imagenes\n",
        "        self.labels = labels            # Variable clase\n",
        "        self.target_size = target_size  # Tamano de los parches\n",
        "                                        # Por defecto (224x224)\n",
        "        self.nb_channels = 3 # Numero de canales de las imagenes. 3 = RGB\n",
        "        self.batch_size = batch_size\n",
        "        self.nb_crops = nb_crops    # Cantidad de parches. \n",
        "        # Por defecto 3 (3 de ancho x 3 de alto = 9 parches totales)\n",
        "        self.indexes = np.arange(self.n_files)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Calcula el numero de batches por epoca\n",
        "        \"\"\"\n",
        "        return int(np.ceil(self.n_files / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Genera un batch de datos\n",
        "        \"\"\"\n",
        "        indexes = self.indexes[index*self.batch_size:min((index+1)*self.batch_size,self.n_files)]\n",
        "\n",
        "        files_batch = self.files[indexes]\n",
        "        labels_batch = self.labels[indexes]\n",
        "        \n",
        "        # Dimension de la variable X de un batch:\n",
        "        # (tamano_batch, cantidad_total_parches, anchura_parches, altura_parches, numero_canales)\n",
        "        # Por defecto --> (32, 9, 224, 224, 3)\n",
        "        X = np.zeros((len(files_batch), self.nb_crops*self.nb_crops, self.target_size[0], self.target_size[1], self.nb_channels), dtype = np.float32)\n",
        "\n",
        "        # Para cada imagen del batch\n",
        "        for i in range(len(files_batch)):\n",
        "          fn = files_batch[i]\n",
        "          img = imread(fn)  # Se carga la imagen\n",
        "\n",
        "          pad_x = 0\n",
        "          pad_y = 0\n",
        "          # Si la imagen es de menor tamano que target_size, se aplica padding para agrandarla\n",
        "          if img.shape[0] <= self.target_size[0]:\n",
        "              pad_y = (self.target_size[0] - img.shape[0]) // 2 + 1 \n",
        "          if img.shape[1] <= self.target_size[1]:\n",
        "              pad_x = (self.target_size[1] - img.shape[1]) // 2 + 1 \n",
        "          # Si no es una imagen a color\n",
        "          if len(img.shape) < 3:\n",
        "              img = np.stack((img,)*3, axis=-1)\n",
        "          img = np.pad(img, ((pad_y,pad_y), (pad_x,pad_x), (0,0)), mode='reflect') \n",
        "\n",
        "          # Guarda el ancho por alto de la imagen\n",
        "          img_x, img_y = img.shape[1], img.shape[0] \n",
        "          # Formula para calcular la distancia entre parches: (distancia_x = tamano_original_x - tamano_parche_x) / x-1\n",
        "          distancia_x = int((img_x - self.target_size[0]) / (self.nb_crops - 1))\n",
        "          distancia_y = int((img_y - self.target_size[1]) / (self.nb_crops - 1))\n",
        "\n",
        "          j=0\n",
        "          for w in range(self.nb_crops):  # Para cada parche\n",
        "            for z in range(self.nb_crops):\n",
        "              # Se guardan las coordenadas de los parches\n",
        "              crop = img[distancia_y*w:distancia_y*w + img_size, distancia_x*z:distancia_x*z + img_size, :].copy()\n",
        "              X[i, j] = crop\n",
        "              j=j+1\n",
        "\n",
        "        return X, labels_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1OiInXUsE0J"
      },
      "source": [
        "Se define el modelo que se encargará de preprocesar las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9oaLoR1nI6q"
      },
      "source": [
        "# Hiperparametros\n",
        "nb_crops = 3\n",
        "img_size = 224\n",
        "nb_channels = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlfUu7B0sGIE"
      },
      "source": [
        "# Dimensiones de entrada --> (9, 224, 224, 3)\n",
        "image_shape = (nb_crops*nb_crops, img_size, img_size, nb_channels)\n",
        "inp = tf.keras.layers.Input(shape=image_shape)\n",
        "\n",
        "prep = tf.keras.applications.mobilenet.preprocess_input\n",
        "core = tf.keras.applications.MobileNet(weights=\"imagenet\", include_top=False, pooling=\"avg\",\n",
        "                                       input_shape=(img_size, img_size, nb_channels))\n",
        "core.trainable = False  # Importante!!! MobileNet se marca como no entrenable\n",
        "\n",
        "e = []\n",
        "for i in range(nb_crops*nb_crops):   # Para cada parche creado\n",
        "  # Se preprocesa; se pasa por mobilenet; se agrega una dimension\n",
        "  e.append(tf.expand_dims(core(prep(inp[:, i])), 1))\n",
        "\n",
        "out = tf.concat(e, axis=1)             # Se pegan los 9 parches\n",
        "\n",
        "coder = Model(inputs=inp, outputs=out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvqYI4hOsHlj"
      },
      "source": [
        "coder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVWZ0U0JsKpi"
      },
      "source": [
        "Hay que pasar todas las imágenes del dataset AVA por este modelo para preprocesarlas. Este es un proceso muy costoso y lleva mucho tiempo.\n",
        "\n",
        "Como el servicio de Google Colab es limitado se ha hecho un bucle for y se han preprocesado las imágenes en grupos de 10.000. Cada uno de estos grupos ha tardado casi una hora en preprocesarse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scupZzCw53Vb"
      },
      "source": [
        "**Importante!!!** Modificar si se desea la dirección en la que se van a almacenar estos archivos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMuoe6HC6IRJ"
      },
      "source": [
        "direccion_archivos = './drive/MyDrive/TFG/data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCrUqCxWn76B"
      },
      "source": [
        "for i in range(0, 26):\n",
        "  x1 = X[i*10000:(i+1)*10000]\n",
        "  y1 = y[i*10000:(i+1)*10000]\n",
        "\n",
        "  # Se define el generador para los datos seleccionados\n",
        "  generator = PatchGenerator(x1, y1)\n",
        "  # Se preprocesan\n",
        "  X_preprocesada = coder.predict_generator(generator)\n",
        "  # Se almacenan los vectores de caracteristicas obtenidos\n",
        "  # en un archivo numpy\n",
        "  np.save(direccion_archivos+'X'+str(i+1), X_preprocesada)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADsBzOZHsUPS"
      },
      "source": [
        "## Construir dataset\n",
        "\n",
        "A partir de los archivos guardados con el resultado del preprocesamiento, se van a guardar los vectores de características de cada imagen (cada imagen tiene 9 vectores, uno por parches). Se guardarán los vectores de cada imagen en un archivo `.npy` distinto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stxoPC4w6x3y"
      },
      "source": [
        "**Importante!!!** Modificar si se desea la dirección en la que se van a almacenar las imágenes preprocesadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcrcxWR26bPP"
      },
      "source": [
        "direccion_datos = './drive/MyDrive/TFG/datos/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_3yhD-SsbCz"
      },
      "source": [
        "# Se crean 27 carpetas (una por cada 10.000 imagenes)\n",
        "for i in range(1,27):\n",
        "  os.mkdir(direccion_datos + 'X' + str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CRO1unGscNg"
      },
      "source": [
        "# Para cada carpeta se almacenan las 10.000 imagenes correspondientes\n",
        "for i in range(0,27):\n",
        "  X = np.load(direccion_archivos+'X'+str(i)+'.npy')\n",
        "  for cont in range(0,len(X)):\n",
        "    np.save(direccion_datos + 'X'+ str(i) +'/'+str(cont)+'.npy', X[cont])\n",
        "\n",
        "  print(\"Iteracion \", i, \". Archivos: \", len(os.listdir(direccion_datos + 'X'+str(i))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQDijgNZCdE3"
      },
      "source": [
        "Todos estos archivos que están guardados en 27 carpetas se pueden comprimir manualmente en 5 archivos `.zip`. Estos archivos comprimidos se utilizarán en la libreta `Modelos.ipynb`, copiándose al entorno de ejecución y descomprimiéndolos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTu0cXjvqM4N"
      },
      "source": [
        "Ahora se va a generar un `.csv` con la dirección que tendrá cada archivo dentro del directorio y la variable clase asociada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAv9W4lXsYLt"
      },
      "source": [
        "# Se genera un dataframe de pandas con la direccion de los archivos y la clase\n",
        "for i in range(0,len(data)):\n",
        "  # La columna id tendra un valor de la forma\n",
        "  # --> Xi/numero.npy\n",
        "  data['id'].loc[i] = 'X' + str(int(i/10000)+1) + '/' + str(i%10000) + '.npy'\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4xKdq0AsaDc"
      },
      "source": [
        "# Se guarda el .csv\n",
        "data.to_csv(direccion_datos + 'dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}